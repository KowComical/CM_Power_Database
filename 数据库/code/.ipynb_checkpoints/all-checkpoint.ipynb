{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "711f0432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, re, os,datetime,calendar,matplotlib.pyplot as plt\n",
    "################################备注########################################\n",
    "#df_cleaned 时间列 = ['datetime','date','year','month','month_date','weekday','hour']\n",
    "#df_simulated_hourly 时间列 = ['datetime','date','year','month','month_date','weekday','hour'] unit = ['Mwh']\n",
    "#df_simulated_daily 时间列 = ['date','year','month','month_date','weekday'] unit = ['Gwh']\n",
    "#df_simulated_monthly 时间列 = ['year','month'] unit = ['Gwh']\n",
    "############################function#########################################\n",
    "def write_pic(file_path, country):\n",
    "    iea = iea_data(country)\n",
    "    result = []\n",
    "    folder = os.listdir(file_path)\n",
    "    for dbtype in folder[::]:\n",
    "        if os.path.isfile(os.path.join(file_path,dbtype)):\n",
    "            folder.remove(dbtype)\n",
    "    for f in folder:\n",
    "        d_file_path = os.path.join(file_path,f)\n",
    "        file = os.listdir(d_file_path)\n",
    "        file_name = []\n",
    "        for dbtype in file:\n",
    "            if os.path.isfile(os.path.join(d_file_path,dbtype)):\n",
    "                file_name.append(dbtype)\n",
    "        file_name = [file_name[i] for i,x in enumerate(file_name) if x.find('monthly')!=-1]\n",
    "        df = pd.read_csv(d_file_path+file_name[0])\n",
    "        result.append(df)\n",
    "    df_all = pd.DataFrame(np.concatenate(result), columns = df.columns)\n",
    "    df_all['date'] = pd.to_datetime(df_all[['year', 'month']].assign(Day=1))#合并年月\n",
    "    iea['date'] = pd.to_datetime(iea[['year', 'month']].assign(Day=1))#合并年月\n",
    "    date_list = sorted(list(set(df_all['date']) & set(iea['date'])))\n",
    "\n",
    "    df_all = df_all.set_index('date').loc[date_list].reset_index()\n",
    "    iea = iea.set_index('date').loc[date_list].reset_index()\n",
    "\n",
    "    font_size = 25\n",
    "    plt.figure(figsize=(25, 25))\n",
    "    i = 1\n",
    "    for z in df_all.columns.tolist():\n",
    "        if z in iea.columns.tolist():\n",
    "            if z != 'year' and z != 'month' and z != 'date':\n",
    "                pic = plt.subplot(4,2, i)\n",
    "                pic.set_title ('[' + str(i) + ']'+ ' '+z, size = font_size)\n",
    "                i += 1\n",
    "                x = df_all['date']\n",
    "                y1 = df_all[z]\n",
    "                y2 = iea[z]\n",
    "                plt.plot(x,y1, color = 'red', label = 'simulated')\n",
    "                plt.plot(x,y2, label = 'iea')\n",
    "                #plt.tick_params(labelsize=font_size)\n",
    "                plt.xlabel('Year', size = font_size)\n",
    "                plt.ylabel('Emissions', size = font_size)\n",
    "                plt.rcParams.update({'font.size': 15})\n",
    "                pic.legend(loc = 0)\n",
    "                plt.tight_layout()\n",
    "    plt.savefig('D:\\\\Python\\\\Work\\\\朱碧青\\\\Image_Store\\\\2022\\\\02-25\\\\'+country+'.png')\n",
    "    \n",
    "def create_folder(file_path,Type): #建立需要的文件夹\n",
    "    out_path = os.path.join(file_path,Type+'\\\\')\n",
    "    if not os.path.exists(out_path): #如果有了文件夹的话就直接pass掉\n",
    "        os.mkdir(out_path)\n",
    "    return out_path\n",
    "def check_col(df,Type): #检查数据的现存列名是否一致 如果不一致就删掉多余的\n",
    "    df_col = df.columns.tolist()\n",
    "    if Type == 'hourly': #如果是simulated_hourly\n",
    "        r_col = ['unit','datetime', 'date','year','month','month_date','weekday','hour','coal','oil','gas','nuclear','hydro','wind','solar','other','fossil','low.carbon','total.prod','coal.perc','oil.perc','gas.perc','nuclear.perc','hydro.perc','wind.perc','solar.perc','other.perc','fossil.perc','low.carbon.perc']\n",
    "        df['unit'] = 'Mwh'\n",
    "    if Type == 'daily':\n",
    "        r_col = ['unit','date','year','month','month_date','weekday','coal','oil','gas','nuclear','hydro','wind','solar','other','fossil','low.carbon','total.prod','coal.perc','oil.perc','gas.perc','nuclear.perc','hydro.perc','wind.perc','solar.perc','other.perc','fossil.perc','low.carbon.perc']\n",
    "        df['unit'] = 'Gwh'\n",
    "    if Type == 'monthly':\n",
    "        r_col = ['unit','year','month','coal','oil','gas','nuclear','hydro','wind','solar','other','fossil','low.carbon','total.prod','coal.perc','oil.perc','gas.perc','nuclear.perc','hydro.perc','wind.perc','solar.perc','other.perc','fossil.perc','low.carbon.perc']\n",
    "        df['unit'] = 'Gwh'\n",
    "    df = df[r_col]\n",
    "    return df\n",
    "def time_b_a(x,which): #根据which 选择得到所选日期的前which天或者后which天\n",
    "    myday = datetime.datetime.strptime(x,'%Y-%m-%d')\n",
    "    delta = datetime.timedelta(days=which)\n",
    "    my_yestoday = myday + delta\n",
    "    my_yes_time = my_yestoday.strftime('%Y-%m-%d')\n",
    "    return my_yes_time    \n",
    "def time_info(df,date_name): #添加各种时间列\n",
    "    df[date_name] = pd.to_datetime(df[date_name])\n",
    "    df['year'] = df[date_name].dt.year\n",
    "    df['month'] = df[date_name].dt.month\n",
    "    df['month_date']  = df[date_name].dt.strftime('%m-%d')\n",
    "    df['weekday'] = df[date_name].dt.day_name()\n",
    "    df['hour'] = df[date_name].dt.hour\n",
    "    if date_name != 'date':\n",
    "        df['date'] = df[date_name].dt.strftime('%Y-%m-%d')\n",
    "def check_date(df,date_name,f): #检查现存时间缺失值并填充\n",
    "    df[date_name] = pd.to_datetime(df[date_name])\n",
    "    real_date = pd.date_range(start=min(df[date_name]),end = max(df[date_name]),freq = f).tolist()\n",
    "    df_date = df[date_name].drop_duplicates().tolist()\n",
    "    missing_date = list(set(real_date)-set(df_date))\n",
    "    for z in missing_date:\n",
    "        df = df.append([{date_name:z}], ignore_index=True)\n",
    "        df = df.sort_values(by = date_name).reset_index(drop = True)\n",
    "    time_info(df,date_name)\n",
    "    return df\n",
    "def insert_date(df,date_name,z):\n",
    "    df[date_name] = pd.to_datetime(df[date_name])\n",
    "    df = df.append([{date_name:z}], ignore_index=True)\n",
    "    df = df.sort_values(by = date_name).reset_index(drop = True)\n",
    "    return df\n",
    "def iea_data(j):\n",
    "    j = j.upper()\n",
    "    iea_path = 'K:\\\\Github\\\\GlobalPowerUpdate-Kow\\\\数据库\\\\data\\\\#global_rf\\\\iea\\\\'\n",
    "    df_iea = pd.read_csv(iea_path+j+'.csv').rename(columns = {'NaturalGas':'gas'})\n",
    "    df_iea.columns = df_iea.columns.map(lambda x:x.lower()) # 全小写\n",
    "    other_list = ['combustiblerenewables', 'geothermal','otherrenewables']\n",
    "    total_list = ['coal','gas','oil','nuclear','hydro','wind','solar','other']\n",
    "    all_list = total_list+['total.gen','year','month']\n",
    "    df_iea['other'] = df_iea[other_list].astype(float).sum(axis = 1)\n",
    "    df_iea['total.gen'] = df_iea[total_list].astype(float).sum(axis = 1)\n",
    "    df_iea = df_iea[all_list]\n",
    "    return df_iea\n",
    "#填充缺失值\n",
    "def fill_null(df,j,date_name,Type):\n",
    "    df_iea = iea_data(j)\n",
    "    filling_result = []\n",
    "    df_null = df[df.isna().any(axis=1)].reset_index(drop = True) #所有包含缺失值的行\n",
    "    df_not_null = df[~df.isna().any(axis=1)].reset_index(drop = True) #所有不包含缺失值的行\n",
    "    for x in df_null['year'].drop_duplicates().tolist(): #按年份循环\n",
    "        df_temp = df_null[df_null['year'] == x].reset_index(drop = True) #按年份赋值新的df\n",
    "        for y in df_temp['month'].drop_duplicates().tolist(): #新df中按月循环\n",
    "            df_temp_month = df_temp[df_temp['month'] == y].reset_index(drop = True) #按月份赋值新的df\n",
    "            month_date = calendar.monthrange(x,y)[1] #计算当月天数\n",
    "            for z in df_iea.columns.tolist(): \n",
    "                if df_iea[z].dtype == float and z != 'total.gen': #如果这一列类型是float并且不为total时\n",
    "                    try:#如果这一列类型是float并且不为total时\n",
    "                        df_temp_month[z] = df_iea[(df_iea['year'] == x) & (df_iea['month'] == y)][z].tolist()[0]/month_date\n",
    "                    except:\n",
    "                        df_temp_month[z] = df_iea[(df_iea['year'] == x-1) & (df_iea['month'] == y)][z].tolist()[0]/month_date\n",
    "            filling_result.append(df_temp_month)\n",
    "    df_missing = pd.DataFrame(np.concatenate(filling_result), columns = df_temp_month.columns)\n",
    "    df = pd.concat([df_missing,df_not_null])\n",
    "    df = df.sort_values(by = date_name).reset_index(drop = True)\n",
    "    total_proc(df, unit = True)\n",
    "    df = check_col(df,Type)\n",
    "    return df\n",
    "def total_proc(df, unit = True): #处理数据\n",
    "    fossil_list = ['coal','gas','oil']\n",
    "    carbon_list = ['nuclear','hydro','wind','solar','other']\n",
    "    perc_list = ['fossil','low.carbon']\n",
    "    df[fossil_list] = df[fossil_list].astype(float)\n",
    "    df[carbon_list] = df[carbon_list].astype(float)\n",
    "    df['fossil'] = df[fossil_list].astype(float).sum(axis = 1)\n",
    "    df['low.carbon'] = df[carbon_list].astype(float).sum(axis = 1)\n",
    "    df['total.prod'] = df[perc_list].astype(float).sum(axis = 1)\n",
    "    if unit == True:\n",
    "        for z in df.columns.tolist():\n",
    "            if df[z].dtype == float:\n",
    "                df[z] = df[z]/1000\n",
    "    for y in df.columns.tolist():\n",
    "        if df[y].dtype == float:\n",
    "            if y != 'total.prod' and 'perc' not in y:\n",
    "                df[y+'.perc'] = df[y]/df['total.prod']          \n",
    "    df['unit'] = 'Gwh' #补充单位\n",
    "    \n",
    "def agg(df,date_name,path,Type,name,folder, unit): #输出\n",
    "    time_info(df,date_name)\n",
    "    total_proc(df, unit)\n",
    "    df = check_col(df,Type)\n",
    "    if folder == True:\n",
    "        out_path = create_folder(path,Type)\n",
    "        out_file = out_path+name\n",
    "    else:\n",
    "        out_file = path+name\n",
    "    df.to_csv(out_file, index = False, encoding = 'utf_8_sig')\n",
    "############################################################################################################################################\n",
    "global_path = 'K:\\\\Github\\\\GlobalPowerUpdate-Kow\\\\数据库\\\\data\\\\'\n",
    "class dp:\n",
    "#################################################################US#########################################################################    \n",
    "    def us():\n",
    "        file_path = os.path.join(global_path,'n_america','us')\n",
    "        in_path = os.path.join(file_path,'raw')\n",
    "        out_path_cleaned  = create_folder(file_path,'cleaned')\n",
    "        out_path_simulated = create_folder(file_path,'simulated')\n",
    "        in_path_file = os.path.join(in_path,'%s.csv' % '20211014_WenliZhao_df_US_format')\n",
    "        in_path_file_2016 = os.path.join(in_path,'%s.csv' % 'US_daily_generation-2016-0.2_20201219')\n",
    "        in_path_file_2017 = os.path.join(in_path,'%s.csv' % 'US_daily_generation-2017-0.2_20201219')\n",
    "        in_path_file_2018 = os.path.join(in_path,'%s.csv' % 'US_daily_generation-2018-0.3_20210521')\n",
    "    #########################################raw-cleaned#################################################\n",
    "        df = pd.read_csv(in_path_file)\n",
    "        df = df.rename(columns = {'Natural gas':'gas','Petroleum':'oil'})\n",
    "        df.columns = df.columns.map(lambda x:x.lower()) # 全小写\n",
    "        df['datetime'] = pd.to_datetime(df[['year', 'month','day','hour']].assign(),errors='coerce')\n",
    "        df = check_date(df,'datetime','h')#检查是否有missing date\n",
    "        df['unit'] = 'Mwh'#单位\n",
    "        time_info(df,'datetime')#日期\n",
    "        year_list = df['year'].drop_duplicates().tolist()\n",
    "        #cleaned-simulated 准备工作\n",
    "        for y in year_list:\n",
    "            df_cleaned = df[df['year'] == y].reset_index(drop = True)\n",
    "            df_cleaned.to_csv(out_path_cleaned+'us-generation-'+str(y)+'-cleaned.csv', index = False, encoding = 'utf_8_sig')\n",
    "        ###########################cleaned-simulated#################################\n",
    "        #hourly\n",
    "            df_hourly = df_cleaned.copy()\n",
    "            out_path_simulated_yearly = create_folder(out_path_simulated,str(y))\n",
    "            agg(df_hourly,'datetime',out_path_simulated_yearly,'hourly',name = 'US_hourly_generation-'+str(y)+'.csv', folder = False, unit = False)\n",
    "            \n",
    "        #填补daily数据中缺失的年份\n",
    "        df_daily = df.set_index('datetime').resample('d').sum().reset_index().drop(columns = ['day','hour'])\n",
    "        time_info(df_daily,'datetime')\n",
    "        total_proc(df_daily, unit = True)\n",
    "        df_16 = pd.read_csv(in_path_file_2016)\n",
    "        df_17 = pd.read_csv(in_path_file_2017)\n",
    "        df_18 = pd.read_csv(in_path_file_2018)\n",
    "        df_partall = pd.concat([df_16,df_17,df_18])\n",
    "        df_partall = df_partall.rename(columns = {'date':'datetime'}).drop(columns = ['season'])\n",
    "        time_info(df_partall,'datetime')\n",
    "        df_daily = df_daily[df_daily['year'] != 2018].reset_index(drop = True)\n",
    "        df_all = pd.concat([df_partall,df_daily]).reset_index(drop = True)\n",
    "        for y in df_all['year'].drop_duplicates().tolist():\n",
    "        #daily\n",
    "            df_daily = df_all[df_all['year'] == y].reset_index(drop = True)\n",
    "            df_monthly = df_daily.copy()\n",
    "            out_path_simulated_yearly = create_folder(out_path_simulated,str(y))\n",
    "            agg(df_daily,'datetime',out_path_simulated_yearly,'daily',name = 'US_daily_generation-'+str(y)+'.csv', folder = False, unit = False)\n",
    "        #monthly\n",
    "            time_info(df_monthly,'datetime')\n",
    "            df_monthly = df_monthly.set_index('datetime').resample('m').sum().reset_index()\n",
    "            out_path_simulated_yearly = create_folder(out_path_simulated,str(y))\n",
    "            agg(df_monthly,'datetime',out_path_simulated_yearly,'monthly',name = 'US_monthly_generation-'+str(y)+'.csv', folder = False, unit = False)\n",
    "##############################################################India#########################################################################\n",
    "    def india():\n",
    "        file_path = os.path.join(global_path,'asia','india')\n",
    "        in_path = os.path.join(file_path,'raw')\n",
    "        out_path_cleaned  = create_folder(file_path,'cleaned')\n",
    "        out_path_simulated = create_folder(file_path,'simulated')\n",
    "        in_path_file = os.path.join(in_path,'%s.csv' % 'India_POSOCO_Daily')\n",
    "        in_path_file_2017 = os.path.join(in_path,'%s.csv' % 'C2017_india_daily_generation-v0.1_20201118')\n",
    "        in_path_file_2018 = os.path.join(in_path,'%s.csv' % 'C2018_india_daily_generation-v0.1_20201118')\n",
    "        ###################################################raw-cleaned########################################################\n",
    "        df = pd.read_csv(in_path_file)\n",
    "        #找出重复的日期值\n",
    "        duplicated_date = df[df.duplicated(['X'])]['X'].tolist()\n",
    "        #将重复日期修改\n",
    "        for x in duplicated_date:\n",
    "            my_yes_time = time_b_a(x,-1)\n",
    "            if df[df['X'] == my_yes_time].empty == False: #如果前一天有值\n",
    "                my_yes_time = time_b_a(x,+1)\n",
    "                if df[df['X'] == my_yes_time].empty == True: #如果后一天没有值 则将这一天改为后一天\n",
    "                    index_back = df[df['X'] == x].index.tolist()\n",
    "                    df.loc[index_back[1],('X')] = my_yes_time\n",
    "            else: #如果前一天没有值 则将这一天改为前一天\n",
    "                index_front = df[df['X'] == x].index.tolist()\n",
    "                df.loc[index_front[0],('X')] = my_yes_time\n",
    "\n",
    "        #查找缺失日期并填充\n",
    "        df = check_date(df,'X','d')\n",
    "        df['date'] = pd.to_datetime(df['X'], format = '%Y-%m-%d')\n",
    "        time_info(df,'X') #添加各种日期数据和单位\n",
    "        df['unit'] = 'MU'\n",
    "        df.columns = df.columns.map(lambda x:x.lower()) # 全小写\n",
    "        df = df.rename(columns = {'gas..naptha...diesel':'gas_naptha_diesel','res..wind..solar..biomass...others.':'res'}).drop(columns = ['unnamed: 0','x']) #修改列名细节\n",
    "\n",
    "        #输出没填充缺失值的\n",
    "        year_list = df['year'].drop_duplicates().tolist()\n",
    "        for x in year_list:\n",
    "            df[df['year'] == x].to_csv(out_path_cleaned+'india-generation-'+str(x)+'-cleaned.csv', index = False, encoding = 'utf_8_sig')\n",
    "\n",
    "        #处理缺失值并输出\n",
    "        df_filled = df.copy()\n",
    "        for x in df_filled.columns.tolist():\n",
    "            if df_filled[x].dtype == float:\n",
    "                df_filled[x] = df_filled[x].fillna(df_filled[x].interpolate())\n",
    "\n",
    "        #################添加2017-2018的df_filled数据#####################\n",
    "        df_17 = pd.read_csv(in_path_file_2017)\n",
    "        df_18 = pd.read_csv(in_path_file_2018)\n",
    "\n",
    "        df_filled = pd.concat([df_17,df_18,df_filled])\n",
    "        df_filled['date'] = pd.to_datetime(df_filled['date'])\n",
    "        time_info(df_filled,'date')\n",
    "        df_filled = df_filled.reset_index(drop = True)\n",
    "        #填充coal这一列\n",
    "        index = df_filled.index.tolist()\n",
    "        for t in index:\n",
    "            if pd.isna(df_filled['coal'].iloc[t]) == True: #当coal没有值的时候\n",
    "                df_filled.loc[t,('coal')] = df_filled.loc[t,('coal_lignite')]\n",
    "        df_filled = df_filled[~df_filled.duplicated()].reset_index(drop = True)\n",
    "\n",
    "        #处理重复值\n",
    "        df_filled = df_filled.sort_values(by='date',na_position='first').reset_index(drop = True)\n",
    "        index = df_filled.index.tolist()\n",
    "        drop_list = []\n",
    "        for t in index:\n",
    "            if pd.isna(df_filled['coal'].iloc[t]) == False and pd.isna(df_filled['coal_lignite'].iloc[t]) == True:#当coal有值 coal_lignite没有值并且前面有相同日期的时候\n",
    "                try:\n",
    "                    if df_filled['date'].iloc[t] == df_filled['date'].iloc[t-1] or df_filled['date'].iloc[t] == df_filled['date'].iloc[t+1]:\n",
    "                        drop_list.append(t)\n",
    "                except:\n",
    "                    pass\n",
    "        df_filled = df_filled.drop(drop_list).reset_index(drop = True)\n",
    "        ####################################################################        \n",
    "        #处理monthly数据做后续备用\n",
    "        df_iea = iea_data('india')\n",
    "        result = []      \n",
    "        for x in year_list:\n",
    "            df_temp = df_filled[df_filled['year'] == x].reset_index(drop = True)\n",
    "            df_temp.to_csv(out_path_cleaned+'india-generation-'+str(x)+'-cleaned-filled.csv', index = False, encoding = 'utf_8_sig')\n",
    "        ##########################################cleaned-simulated###################################################\n",
    "        for y in df_iea['year'].drop_duplicates().tolist():\n",
    "            df_iea_temp = df_iea[df_iea['year'] == y].reset_index(drop = True)\n",
    "            df_temp = df_filled[df_filled['year'] == y].reset_index(drop = True)\n",
    "            month_range = df_iea_temp['month'].drop_duplicates().tolist()\n",
    "            if month_range != 12:\n",
    "                df_iea_temp = df_iea[df_iea['year'] == y-1].reset_index(drop = True)\n",
    "                df_temp = df[df['year'] == y].reset_index(drop = True)\n",
    "                month_range = df_iea_temp['month'].drop_duplicates().tolist()\n",
    "            for i in month_range:\n",
    "                df_temp_monthly = df_temp[df_temp['month'] == i].reset_index(drop = True)\n",
    "\n",
    "                gas_value = df_iea_temp[df_iea_temp['month'] == i]['gas'].tolist()[0]\n",
    "                oil_value = df_iea_temp[df_iea_temp['month'] == i]['oil'].tolist()[0]\n",
    "                df_temp_monthly['gas'] = df_temp_monthly['gas_naptha_diesel']*(gas_value/(gas_value+oil_value))\n",
    "                df_temp_monthly['oil'] = df_temp_monthly['gas_naptha_diesel']*(oil_value/(gas_value+oil_value))\n",
    "\n",
    "                solar_value = df_iea_temp[df_iea_temp['month'] == i]['solar'].tolist()[0]\n",
    "                wind_value = df_iea_temp[df_iea_temp['month'] == i]['wind'].tolist()[0]\n",
    "                other_value = df_iea_temp[df_iea_temp['month'] == i]['other'].tolist()[0]\n",
    "                df_temp_monthly['solar'] = df_temp_monthly['res']*(solar_value/(solar_value+wind_value+other_value))\n",
    "                df_temp_monthly['wind'] = df_temp_monthly['res']*(wind_value/(solar_value+wind_value+other_value))\n",
    "                df_temp_monthly['other'] = df_temp_monthly['res']*(other_value/(solar_value+wind_value+other_value))\n",
    "                result.append(df_temp_monthly)\n",
    "\n",
    "        df_all = pd.DataFrame(np.concatenate(result), columns = df_temp_monthly.columns)\n",
    "\n",
    "        df_all['coal'] = (df_all['coal']+df_all['lignite'])\n",
    "        for t in df_all.index.tolist():\n",
    "            if pd.isna(df_all['coal'].iloc[t]) == True and pd.isna(df_all['lignite'].iloc[t]) == False:\n",
    "                df_all.loc[t,('coal')] = df_all.loc[t,('lignite')]\n",
    "\n",
    "        #生成缺失的日期\n",
    "        start_range = str(min(df_iea['year']))+'-01-01' #从monthly里面最小年份开始\n",
    "        end_range = min(df_all['date']) #从monthly里面最小年份开始一直到df_all里最小日期截至\n",
    "        date_range = pd.date_range(start=start_range ,end=end_range ,freq = 'd')[:-1] #去掉最后一天\n",
    "\n",
    "        for x in date_range:\n",
    "            df_all = insert_date(df_all,'date',x)\n",
    "\n",
    "        df_all = check_date(df_all,'date','d')#填补缺失的日期\n",
    "        total_list = ['coal','gas','oil','nuclear','hydro','wind','solar','other']\n",
    "        df_all[total_list] = df_all[total_list].astype(float)\n",
    "        df_all[total_list] = df_all[total_list].interpolate()\n",
    "\n",
    "        #填充缺失值\n",
    "        df_all = fill_null(df_all,'india','date','daily')\n",
    "        df_all[total_list] = df_all[total_list]*1000\n",
    "        ##########################################################daily#############################################\n",
    "        for y in df_all['year'].drop_duplicates().tolist():\n",
    "            df_temp = df_all[df_all['year'] == y]\n",
    "            df_daily = df_temp.copy()\n",
    "            df_monthly= df_temp.copy()\n",
    "            out_path_simulated_yearly = create_folder(out_path_simulated,str(y))\n",
    "            #daily\n",
    "            agg(df_daily,'date',out_path_simulated_yearly,'daily',name = 'India_daily_generation-'+str(y)+'.csv', folder = False, unit = False)\n",
    "        #########################################################monthly##########################################    \n",
    "            #monthly\n",
    "            df_monthly = df_monthly.set_index('date').resample('m').sum().reset_index()\n",
    "            agg(df_monthly,'date',out_path_simulated_yearly,'monthly',name = 'India_monthly_generation-'+str(y)+'.csv', folder = False, unit = False)\n",
    "##########################################################################################################################################            \n",
    "    def brazil():\n",
    "            ###############################################################路径##########################################################\n",
    "        file_path = os.path.join(global_path,'s_america','brazil')\n",
    "        in_path = os.path.join(file_path,'raw')\n",
    "        out_path_cleaned  = create_folder(file_path,'cleaned')\n",
    "        out_path_simulated = create_folder(file_path,'simulated')\n",
    "        in_path_file = os.path.join(in_path,'Brazil_ONS_Hourly.csv')\n",
    "        ##############################################################Raw-Cleaned########################################################\n",
    "        df = pd.read_csv(in_path_file).drop(columns = ['Unnamed: 0']) #存疑\n",
    "        df['Date'] = pd.to_datetime(df['Date'], format = \"%d/%m/%Y %H:%M\",errors='coerce')\n",
    "        df = df[~df[['Date']].isnull().T.any()].rename(columns = {'Date':'datetime'}).reset_index(drop = True) #去除错误行\n",
    "        df = df.drop_duplicates().reset_index(drop = True) #去除重复行\n",
    "\n",
    "        df = check_date(df,'datetime','h')#判断是否有缺失日期\n",
    "        time_info(df,'datetime')#填充时间列\n",
    "\n",
    "        #cleaned 输出\n",
    "        for y in df['year'].drop_duplicates().tolist():\n",
    "            df_cleaned = df[df['year'] == y]\n",
    "            df_cleaned.to_csv(out_path_cleaned+'brazil-generation-'+str(y)+'-cleaned.csv', index = False)\n",
    "        ########################################################cleaned-simulated######################################\n",
    "            df_hourly = df[df['year'] == y].reset_index(drop = True).fillna(0)\n",
    "            df_hourly['Thermal.Gás.natural'] = df_hourly['Thermal.Gás.natural'].astype(float)+df_hourly['Thermal.Gás.Natural'].astype(float)\n",
    "            df_hourly = df_hourly.drop(columns = ['Thermal.Gás.Natural'])\n",
    "            df_hourly.columns = df_hourly.columns.map(lambda x:x.lower()) # 全小写\n",
    "            df_col = df_hourly.columns.tolist()\n",
    "            coal_list = [df_col[i] for i,x in enumerate(df_col) if x.find('carv')!=-1]\n",
    "            gas_list = [df_col[i] for i,x in enumerate(df_col) if x.find('gás')!=-1]\n",
    "            oil_list = [df_col[i] for i,x in enumerate(df_col) if x.find('leo')!=-1]\n",
    "            other_list = ['thermal.biomassa','thermal.resíduos.industriais']\n",
    "            df_hourly['coal'] = df_hourly[coal_list].astype(float).sum(axis = 1)\n",
    "            df_hourly['gas'] = df_hourly[gas_list].astype(float).sum(axis = 1)\n",
    "            df_hourly['oil'] = df_hourly[oil_list].astype(float).sum(axis = 1)\n",
    "            type_list = ['nuclear','hydro','wind','solar']\n",
    "            df_hourly[type_list] = df_hourly[type_list].astype(float)\n",
    "            df_hourly['other'] = df_hourly[other_list].astype(float).sum(axis = 1)\n",
    "            df_col = df_hourly.columns.tolist()\n",
    "            all_list = [df_col[i] for i,x in enumerate(df_col) if not x.find('thermal')!=-1]\n",
    "            df_hourly = df_hourly[all_list].groupby(all_list).sum().reset_index()\n",
    "            df_daily = df_hourly.copy()\n",
    "            df_monthly = df_hourly.copy()\n",
    "            out_path_simulated_yearly = create_folder(out_path_simulated,str(y))\n",
    "            #hourly\n",
    "            agg(df_hourly,'datetime',out_path_simulated_yearly,'hourly',name = 'Brazil_hourly_generation-'+str(y)+'.csv', folder = False, unit = False)\n",
    "            ##############################################################daily#########################################\n",
    "            #daily\n",
    "            df_daily = df_daily.set_index('datetime').resample('d').sum().reset_index()\n",
    "            agg(df_daily,'datetime',out_path_simulated_yearly,'daily',name = 'Brazil_daily_generation-'+str(y)+'.csv',folder = False, unit = False)\n",
    "           ##############################################monthly############################################### \n",
    "            #monthly\n",
    "            df_monthly = df_monthly.set_index('datetime').resample('m').sum().reset_index()\n",
    "            agg(df_monthly,'datetime',out_path_simulated_yearly,'monthly',name = 'Brazil_monthly_generation-'+str(y)+'.csv',folder = False, unit = False)\n",
    "############################################################################################################################################        \n",
    "    def eu():\n",
    "        file_path = os.path.join(global_path,'europe','eu27_uk')\n",
    "        in_path_entsoe = os.path.join(file_path,'raw','entsoe')\n",
    "        in_path_bmrs = os.path.join(file_path,'raw','uk-BMRS')\n",
    "        out_path_cleaned  = create_folder(file_path,'cleaned')\n",
    "        out_path_simulated = create_folder(file_path,'simulated')\n",
    "        in_path_bmrs_file = os.path.join(file_path,'raw','uk-BMRS','UK_BMRS_Hourly.csv')\n",
    "        ################################################################################################################################\n",
    "        coal_list = ['Fossil Brown coal/Lignite  - Actual Aggregated [MW]',\n",
    "                                 'Fossil Coal-derived gas  - Actual Aggregated [MW]',\n",
    "                                 'Fossil Hard coal  - Actual Aggregated [MW]',\n",
    "                                 'Fossil Peat  - Actual Aggregated [MW]']\n",
    "        gas_list = ['Fossil Gas  - Actual Aggregated [MW]']\n",
    "        oil_list = ['Fossil Oil  - Actual Aggregated [MW]',\n",
    "                                'Fossil Oil shale  - Actual Aggregated [MW]']\n",
    "        other_list = ['Biomass  - Actual Aggregated [MW]',\n",
    "                                  'Geothermal  - Actual Aggregated [MW]',\n",
    "                                 'Other  - Actual Aggregated [MW]',\n",
    "                                 'Other renewable  - Actual Aggregated [MW]',\n",
    "                                 'Waste  - Actual Aggregated [MW]']\n",
    "        hydro_list = ['Hydro Pumped Storage  - Actual Aggregated [MW]',\n",
    "                                 'Hydro Pumped Storage  - Actual Consumption [MW]',\n",
    "                                 'Hydro Run-of-river and poundage  - Actual Aggregated [MW]',\n",
    "                                 'Hydro Water Reservoir  - Actual Aggregated [MW]']\n",
    "        wind_list = ['Wind Offshore  - Actual Aggregated [MW]',\n",
    "                                'Wind Onshore  - Actual Aggregated [MW]']\n",
    "        #################################################################################################################\n",
    "        file = os.listdir(in_path_entsoe)\n",
    "        file_name = []\n",
    "        for dbtype in file:\n",
    "            if os.path.isfile(os.path.join(in_path_entsoe,dbtype)):\n",
    "                file_name.append(dbtype)\n",
    "        #################################################raw-cleaned-simulated_bmrs##########################################\n",
    "        #bmrs\n",
    "        df_bmrs = pd.read_csv(in_path_bmrs_file)\n",
    "        time_info(df_bmrs,'startTimeOfHalfHrPeriod')\n",
    "        for y in df_bmrs['year'].drop_duplicates().tolist():\n",
    "            df_bmrs_cleaned_yearly = df_bmrs[df_bmrs['year'] == y].reset_index(drop = True)\n",
    "            df_hourly = df_bmrs_cleaned_yearly.copy()\n",
    "            out_path_cleaned_yearly = create_folder(out_path_cleaned,str(y))\n",
    "            df_bmrs_cleaned_yearly.to_csv(out_path_cleaned_yearly+'United_Kingdom_BMRS.csv', index = False, encoding = 'utf_8_sig')\n",
    "\n",
    "        df_bmrs['gas'] = df_bmrs['ccgt']+df_bmrs['ocgt']\n",
    "        df_bmrs['hydro'] = df_bmrs['ps']+df_bmrs['npshyd']\n",
    "        df_bmrs['other'] = df_bmrs['biomass']+df_bmrs['other']\n",
    "        df_bmrs['solar'] = 0 #存疑\n",
    "\n",
    "        df_bmrs['datetime'] = pd.to_datetime(df_bmrs['startTimeOfHalfHrPeriod']) + pd.to_timedelta((df_bmrs['settlementPeriod']/2-0.5), unit='h')\n",
    "\n",
    "        df_bmrs = df_bmrs.set_index('datetime').resample('H').mean().reset_index()\n",
    "        time_info(df_bmrs,'datetime')\n",
    "        for y in df_bmrs['year'].drop_duplicates().tolist():\n",
    "            #simulated准备工作\n",
    "            df_bmrs_hourly = df_bmrs[df_bmrs['year'] == y].reset_index(drop = True)\n",
    "            df_bmrs_daily = df_bmrs_hourly.copy()\n",
    "            df_bmrs_monthly = df_bmrs_hourly.copy()\n",
    "            out_path_simulated_yearly = create_folder(out_path_simulated,str(y))\n",
    "            out_path_simulated_yearly_per = create_folder(out_path_simulated_yearly,'per_country')\n",
    "        #hourly\n",
    "            agg(df_bmrs_hourly,'datetime',out_path_simulated_yearly_per,'hourly',name = 'United_Kingdom_BMRS.csv', folder = True, unit = False)\n",
    "        #daily\n",
    "            df_bmrs_daily = df_bmrs_daily.set_index('datetime').resample('d').sum().reset_index()\n",
    "            agg(df_bmrs_daily,'datetime',out_path_simulated_yearly_per,'daily','United_Kingdom_BMRS.csv', folder = True, unit = True)\n",
    "        #monthly\n",
    "            df_bmrs_monthly = df_bmrs_monthly.set_index('datetime').resample('m').sum().reset_index()\n",
    "            agg(df_bmrs_monthly,'datetime',out_path_simulated_yearly_per,'monthly','United_Kingdom_BMRS.csv', folder = True, unit = True)\n",
    "        #########################################entose-raw-cleaned-simulated#############################################\n",
    "        for x in file_name:\n",
    "            df_cleaned = pd.read_csv(in_path_entsoe+'\\\\'+x).rename(columns = {'MTU':'datetime'})\n",
    "            time_info(df_cleaned,'datetime')\n",
    "            for y in df_cleaned['year'].drop_duplicates().tolist():\n",
    "                df_cleaned_yearly = df_cleaned[df_cleaned['year'] == y].reset_index(drop = True)\n",
    "                df_hourly = df_cleaned_yearly.copy()\n",
    "                out_path_cleaned_yearly = create_folder(out_path_cleaned,str(y))\n",
    "                df_cleaned_yearly.to_csv(out_path_cleaned_yearly+x, index = False, encoding = 'utf_8_sig')   \n",
    "        #######simulated 准备工作\n",
    "                df_hourly['coal'] = df_hourly[coal_list].astype(float).sum(axis = 1)\n",
    "                df_hourly['oil'] = df_hourly[oil_list].astype(float).sum(axis = 1)\n",
    "                df_hourly['gas'] = df_hourly[gas_list].astype(float).sum(axis = 1)\n",
    "                df_hourly['nuclear'] = df_hourly['Nuclear  - Actual Aggregated [MW]']\n",
    "                df_hourly['hydro'] = df_hourly[hydro_list].astype(float).sum(axis = 1)\n",
    "                df_hourly['wind'] = df_hourly[wind_list].astype(float).sum(axis = 1)\n",
    "                df_hourly['solar'] = df_hourly['Solar  - Actual Aggregated [MW]']\n",
    "                df_hourly['other'] = df_hourly[other_list].astype(float).sum(axis = 1)\n",
    "\n",
    "                df_col = df_hourly.columns.tolist()\n",
    "                all_list =  [df_col[i] for i,x in enumerate(df_col) if not x.find('MW')!=-1]\n",
    "                df_hourly = df_hourly[all_list]\n",
    "                df_daily = df_hourly.copy()\n",
    "                df_monthly = df_hourly.copy()\n",
    "                out_path_simulated_yearly = create_folder(out_path_simulated,str(y))\n",
    "                out_path_simulated_yearly_per = create_folder(out_path_simulated_yearly,'per_country')\n",
    "        #####hourly\n",
    "                agg(df_hourly,'datetime',out_path_simulated_yearly_per,'hourly',x, folder = True, unit = False)\n",
    "        #####daily\n",
    "                df_daily = df_daily.set_index('datetime').resample('d').sum().reset_index()\n",
    "                agg(df_daily,'datetime',out_path_simulated_yearly_per,'daily',x, folder = True, unit = True)\n",
    "        #####monthly\n",
    "                df_monthly = df_monthly.set_index('datetime').resample('m').sum().reset_index()\n",
    "                agg(df_monthly,'datetime',out_path_simulated_yearly_per,'monthly',x, folder = True, unit = True)\n",
    "###############################################################################################################################################                \n",
    "    def japan():\n",
    "        ################################################################\n",
    "        file_path = os.path.join(global_path,'asia','japan')\n",
    "        in_path = os.path.join(file_path,'raw')\n",
    "        out_path_cleaned  = create_folder(file_path,'cleaned')\n",
    "        out_path_simulated = create_folder(file_path,'simulated')\n",
    "        in_path_file = os.path.join(in_path,'occto.csv')\n",
    "        in_path_file_2016 = os.path.join(in_path,'Japan_daily_generation-2016-v0.1_20201221.csv')\n",
    "        in_path_file_2017 = os.path.join(in_path,'Japan_daily_generation-2017-v0.1_20201221.csv')\n",
    "        in_path_file_2018 = os.path.join(in_path,'Japan_daily_generation-demand-2018-0.1_20201221.csv')\n",
    "##########################################################        \n",
    "        df = pd.read_csv(in_path_file)\n",
    "        #添加各种日期和单位\n",
    "        df['hour'] = df['時刻'].str.replace('時','').astype(int) #将hour列整理出来\n",
    "        df['date'] = pd.to_datetime(df['月日'])\n",
    "        df['datetime'] = pd.to_datetime(df['date']) + pd.to_timedelta((df['hour']), unit='h') #生成datetime\n",
    "\n",
    "        df['year'] = df['datetime'].dt.year\n",
    "        df['month'] = df['datetime'].dt.month\n",
    "        df['month_date']  = df['datetime'].dt.strftime('%m-%d')\n",
    "\n",
    "        df['unit'] = 'Mwh'\n",
    "        df = df.drop(columns = ['月日','時刻'])\n",
    "\n",
    "        #改列名为英文\n",
    "        col_list = ['demand','nuclear','geothermal','hydroelectric','fossil_fuel_fired','biomass','wind','wind_regulated','photovoltaic',\n",
    "                    'photovoltaic_regulated','pumped_storage_hydroelectricity',\n",
    "                    'hour','date','datetime','year', 'month','month_date','unit']\n",
    "        df.columns = col_list\n",
    "\n",
    "        #输出\n",
    "        year_list = df['year'].drop_duplicates().tolist()\n",
    "        for x in year_list:\n",
    "            df[df['year'] == x].to_csv(out_path_cleaned+'japan-generation-'+str(x)+'-cleaned.csv', index = False, encoding = 'utf_8_sig') \n",
    "        ###################################cleaned-simulated######################################################\n",
    "        #iea数据\n",
    "        df_iea = iea_data('japan')\n",
    "        result = []\n",
    "        for y in df_iea['year'].drop_duplicates().tolist():\n",
    "            df_iea_temp = df_iea[df_iea['year'] == y].reset_index(drop = True)\n",
    "            df_temp = df[df['year'] == y].reset_index(drop = True)\n",
    "            month_range = df_iea_temp['month'].drop_duplicates().tolist()\n",
    "            if month_range != 12:\n",
    "                df_iea_temp = df_iea[df_iea['year'] == y-1].reset_index(drop = True)\n",
    "                df_temp = df[df['year'] == y].reset_index(drop = True)\n",
    "                month_range = df_iea_temp['month'].drop_duplicates().tolist()\n",
    "            for i in month_range:\n",
    "                df_temp_monthly = df_temp[df_temp['month'] == i].reset_index(drop = True)\n",
    "\n",
    "                coal_value = df_iea_temp[df_iea_temp['month'] == i]['coal'].tolist()[0]\n",
    "                gas_value = df_iea_temp[df_iea_temp['month'] == i]['gas'].tolist()[0]\n",
    "                oil_value = df_iea_temp[df_iea_temp['month'] == i]['oil'].tolist()[0]\n",
    "\n",
    "                df_temp_monthly['coal'] = df_temp_monthly['fossil_fuel_fired']*(coal_value/(coal_value+gas_value+oil_value))\n",
    "                df_temp_monthly['gas'] = df_temp_monthly['fossil_fuel_fired']*(gas_value/(coal_value+gas_value+oil_value))\n",
    "                df_temp_monthly['oil'] = df_temp_monthly['fossil_fuel_fired']*(oil_value/(coal_value+gas_value+oil_value))\n",
    "\n",
    "                df_temp_monthly['hydro'] = df_temp_monthly['hydroelectric']+df_temp_monthly['pumped_storage_hydroelectricity']\n",
    "                df_temp_monthly['solar'] = df_temp_monthly['photovoltaic']+df_temp_monthly['photovoltaic_regulated']\n",
    "                df_temp_monthly['wind'] = df_temp_monthly['wind']+df_temp_monthly['wind_regulated']\n",
    "                df_temp_monthly['other'] = df_temp_monthly['biomass']+df_temp_monthly['geothermal']\n",
    "                result.append(df_temp_monthly)\n",
    "        df_all = pd.DataFrame(np.concatenate(result), columns = df_temp_monthly.columns)\n",
    "\n",
    "        for x in df_all.columns.tolist():\n",
    "            try:\n",
    "                df_all[x] = df_all[x].astype(float)\n",
    "            except:\n",
    "                pass\n",
    "        time_info(df_all,'datetime')\n",
    "        for y in df_all['year'].drop_duplicates().tolist():\n",
    "            df_hourly = df_all[df_all['year'] == y].reset_index(drop = True)\n",
    "            df_daily = df_hourly.copy()\n",
    "            df_monthly = df_hourly.copy()\n",
    "            out_path_simulated_yearly = create_folder(out_path_simulated,str(y))\n",
    "            #hourly\n",
    "            agg(df_hourly,'datetime',out_path_simulated_yearly,'hourly',name = 'Japan_hourly_generation-'+str(y)+'.csv', folder = False, unit = True)\n",
    "            #daily\n",
    "            df_daily = df_daily.set_index('datetime').resample('d').sum().reset_index()\n",
    "            agg(df_daily,'datetime',out_path_simulated_yearly,'daily',name = 'Japan_daily_generation-'+str(y)+'.csv',folder = False, unit = True)\n",
    "            #monthly\n",
    "            df_monthly = df_monthly.set_index('datetime').resample('m').sum().reset_index()\n",
    "            agg(df_monthly,'datetime',out_path_simulated_yearly,'monthly',name = 'Japan_monthly_generation-'+str(y)+'.csv',folder = False, unit = True)\n",
    "\n",
    "        #daily-simulated 16-18年数据\n",
    "        df_16 = pd.read_csv(in_path_file_2016)\n",
    "        df_17 = pd.read_csv(in_path_file_2017)\n",
    "        df_18 = pd.read_csv(in_path_file_2018).drop(columns = ['total.demand'])\n",
    "        df_p = pd.concat([df_16,df_17,df_18]).reset_index(drop = True)\n",
    "        df_p = check_date(df_p,'date','d') #填充缺失日期\n",
    "        df_p = fill_null(df_p,'japan','date','daily')\n",
    "        for x in df_p.columns.tolist():\n",
    "            if df_p[x].dtype == float:\n",
    "                df_p[x] = df_p[x]*1000\n",
    "\n",
    "        for y in df_p['year'].drop_duplicates().tolist():\n",
    "            df_daily = df_p[df_p['year'] == y].reset_index(drop = True)\n",
    "            df_monthly = df_daily.copy()\n",
    "            out_path_simulated_yearly = create_folder(out_path_simulated,str(y))\n",
    "            #daily\n",
    "            agg(df_daily,'date',out_path_simulated_yearly,'daily',name = 'Japan_daily_generation-'+str(y)+'.csv',folder = False, unit = False)\n",
    "            #monthly\n",
    "            df_monthly = df_monthly.set_index('date').resample('m').sum().reset_index()\n",
    "            agg(df_monthly,'date',out_path_simulated_yearly,'monthly',name = 'Japan_monthly_generation-'+str(y)+'.csv',folder = False, unit = False)\n",
    "\n",
    "    def russia():\n",
    "        file_path = os.path.join(global_path,'europe','russia')\n",
    "        in_path = os.path.join(file_path,'raw')\n",
    "        out_path_cleaned  = create_folder(file_path,'cleaned')\n",
    "        out_path_simulated = create_folder(file_path,'simulated')\n",
    "        in_path_file = os.path.join(in_path,'Russia_SOUPS_Hourly (Corrected).csv')\n",
    "\n",
    "        df = pd.read_csv(in_path_file)\n",
    "        total_list = ['nuclear','hydro','fossil','P_BS','renewables']\n",
    "        df.columns = ['date','nuclear','hydro','fossil','P_BS','renewables']\n",
    "        df['total.prod'] = df[total_list].sum(axis = 1)\n",
    "        df['low.carbon'] = df['total.prod'] - df['fossil']\n",
    "        for z in df.columns.tolist():\n",
    "            if df[z].dtype == float:\n",
    "                df[z] = df[z]/1000\n",
    "        for y in df.columns.tolist():\n",
    "            if df[y].dtype == float:\n",
    "                if y != 'total.prod' and 'perc' not in y:\n",
    "                    df[y+'.perc'] = df[y]/df['total.prod']\n",
    "        df['unit'] = 'Gwh'\n",
    "\n",
    "        #daily\n",
    "        time_info(df,'date')\n",
    "        df = df.drop(columns = ['hour'])\n",
    "        for x in df['year'].drop_duplicates().tolist():\n",
    "            out_path_simulated_yearly = create_folder(out_path_simulated,str(x))\n",
    "            df[df['year'] == x].to_csv(out_path_simulated_yearly+'Russia_daily_generation-'+str(x)+'.csv', index = False, encoding = 'utf_8_sig')\n",
    "\n",
    "        #monthly\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df = df.set_index('date').resample('m').sum().reset_index()\n",
    "        df['year'] = df['date'].dt.year\n",
    "        df['month'] = df['date'].dt.month\n",
    "        for y in df.columns.tolist():\n",
    "            if df[y].dtype == float:\n",
    "                if y != 'total.prod' and 'perc' not in y:\n",
    "                    df[y+'.perc'] = df[y]/df['total.prod']\n",
    "        df['unit'] = 'Gwh'\n",
    "        df = df.drop(columns = ['date'])\n",
    "        for x in df['year'].drop_duplicates().tolist():\n",
    "            out_path_simulated_yearly = create_folder(out_path_simulated,str(x))\n",
    "            df[df['year'] == x].to_csv(out_path_simulated_yearly+'Russia_monthly_generation-'+str(x)+'.csv', index = False, encoding = 'utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "354a9311",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.us()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0415348",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.india()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31330b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kow\\AppData\\Local\\Temp/ipykernel_9348/3332307692.py:1: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11,12,13,14,15,16) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  dp.brazil()\n"
     ]
    }
   ],
   "source": [
    "dp.brazil()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "979c1bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.eu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2512c0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.japan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b073af29",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.russia()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e49a9e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files are the same\n",
      "files are the same\n",
      "files are the same\n",
      "files are the same\n",
      "files are the same\n",
      "files are the same\n",
      "files are the same\n",
      "files are the same\n",
      "files are the same\n",
      "files are the same\n",
      "files are the same\n",
      "files are the same\n"
     ]
    }
   ],
   "source": [
    "#比对文件是否一致\n",
    "## 根目录\n",
    "file_1 = 'D:\\\\Python\\\\Work\\\\朱碧青\\\\Code_Store\\\\数据库\\\\data\\\\asia\\\\india\\\\simulated\\\\'\n",
    "file_2 = 'K:\\\\Github\\\\GlobalPowerUpdate-Kow\\\\数据库\\\\data\\\\asia\\\\india\\\\simulated\\\\'\n",
    "# 三个参数：父目录；所有文件夹名（不含路径）；所有文件名\n",
    "all_file_1 = []\n",
    "for parent, dirnames, filenames in os.walk(file_1):\n",
    "    for fn in filenames:\n",
    "        all_file_1.append(os.path.join(parent, fn))\n",
    "        \n",
    "all_file_2 = []\n",
    "for parent, dirnames, filenames in os.walk(file_2):\n",
    "    for fn in filenames:\n",
    "        all_file_2.append(os.path.join(parent, fn))\n",
    "        \n",
    "for x,y in zip(all_file_1,all_file_2):\n",
    "    try:\n",
    "        status = filecmp.cmp(x, y)\n",
    "    # 为True表示两文件相同\n",
    "        if status:\n",
    "            print(\"files are the same\")\n",
    "    # 为False表示文件不相同\n",
    "        else:\n",
    "            print(\"files are different\")\n",
    "# 如果两边路径头文件不都存在，抛异常\n",
    "    except IOError:\n",
    "        print(\"Error:\"+ \"File not found or failed to read\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
